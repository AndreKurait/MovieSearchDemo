apiVersion: batch/v1
kind: Job
metadata:
  name: load-app-logs
  namespace: movie-demo
spec:
  ttlSecondsAfterFinished: 3600
  backoffLimit: 20
  completions: 10
  parallelism: 10
  completionMode: Indexed
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: log-generator
        image: python:3.11-slim
        env:
        - name: ES_URL
          value: "http://elasticsearch.movie-demo.svc.cluster.local:9200"
        - name: TOTAL_TARGET_DOCS
          value: "3430000000"
        - name: BATCH_SIZE
          value: "5000"
        - name: THREADS
          value: "4"
        - name: THROTTLE_SLEEP_MS
          value: "0"
        command: ["python", "-u", "-c"]
        args:
        - |
          import os, json, time, random, hashlib, sys
          from urllib.request import Request, urlopen
          from urllib.error import URLError

          ES = os.environ["ES_URL"]
          TOTAL_TARGET = int(os.environ.get("TOTAL_TARGET_DOCS", "3430000000"))
          BATCH = int(os.environ.get("BATCH_SIZE", "3000"))
          THREADS = int(os.environ.get("THREADS", "2"))
          THROTTLE_MS = int(os.environ.get("THROTTLE_SLEEP_MS", "50"))
          WORKER = int(os.environ.get("JOB_COMPLETION_INDEX", "0"))
          COMPLETIONS = int(os.environ.get("JOB_COMPLETIONS", "10"))
          INDEX = "application-logs"

          SERVICES = ["api-gateway", "movie-service", "user-service", "search-service",
                      "recommendation-engine", "auth-service", "streaming-service",
                      "payment-service", "notification-service", "cdn-edge"]
          LEVELS = ["INFO"] * 70 + ["WARN"] * 15 + ["ERROR"] * 10 + ["DEBUG"] * 5
          METHODS = ["GET"] * 60 + ["POST"] * 25 + ["PUT"] * 10 + ["DELETE"] * 5
          PATHS = [
              "/api/v1/movies", "/api/v1/movies/{id}", "/api/v1/search",
              "/api/v1/users/{id}/profile", "/api/v1/users/{id}/watchlist",
              "/api/v1/stream/{id}/start", "/api/v1/stream/{id}/heartbeat",
              "/api/v1/auth/login", "/api/v1/auth/refresh", "/api/v1/auth/logout",
              "/api/v1/recommendations/{id}", "/api/v1/movies/{id}/reviews",
              "/api/v1/payments/subscribe", "/api/v1/notifications/preferences",
              "/api/v1/movies/{id}/similar", "/api/v1/genres", "/api/v1/trending",
              "/health", "/metrics", "/api/v1/users/{id}/history",
          ]
          STATUS_CODES = [200] * 75 + [201] * 5 + [204] * 3 + [301] * 2 + \
                         [400] * 5 + [401] * 3 + [403] * 2 + [404] * 3 + [500] * 2
          MESSAGES = {
              "INFO": [
                  "Request completed successfully", "Cache hit for resource",
                  "User session validated", "Stream chunk delivered",
                  "Search query executed in {ms}ms", "Database query returned {n} results",
                  "Rate limit check passed", "JWT token validated",
                  "CDN cache populated", "Recommendation model scored {n} candidates",
              ],
              "WARN": [
                  "Slow query detected: {ms}ms", "Rate limit approaching threshold",
                  "Connection pool utilization at {pct}%",
                  "Retry attempt {n} for upstream service",
                  "Cache miss, falling back to database", "Deprecated API version called",
                  "High memory utilization detected",
              ],
              "ERROR": [
                  "Upstream service timeout after {ms}ms", "Database connection failed",
                  "Authentication token expired",
                  "Internal server error: null pointer exception",
                  "Circuit breaker opened for {svc}",
                  "Failed to process payment transaction", "Stream buffer overflow",
              ],
              "DEBUG": [
                  "Entering request handler", "SQL: SELECT * FROM movies WHERE id = {n}",
                  "Redis GET key=session:{rid}",
                  "HTTP client pool stats: active={n}, idle={n2}",
                  "Request headers parsed",
              ],
          }
          AGENTS = [
              "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
              "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
              "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X)",
              "Mozilla/5.0 (Linux; Android 14) AppleWebKit/537.36",
              "MovieApp/3.2.1 (iOS 17.0)", "MovieApp/3.2.0 (Android 14)",
              "python-requests/2.31.0", "curl/8.4.0",
          ]

          rng = random.Random(42 + WORKER)
          from datetime import datetime, timedelta
          base_ts = datetime(2025, 11, 1) + timedelta(days=WORKER * 30)

          def make_log(seq):
              ts = base_ts + timedelta(milliseconds=seq * rng.randint(5, 50))
              level = rng.choice(LEVELS)
              svc = rng.choice(SERVICES)
              method = rng.choice(METHODS)
              path = rng.choice(PATHS).replace("{id}", str(rng.randint(1, 100000)))
              status = rng.choice(STATUS_CODES)
              rt = rng.randint(1, 200) if status < 400 else rng.randint(200, 5000)
              msg = rng.choice(MESSAGES[level])
              msg = msg.replace("{ms}", str(rt)).replace("{n}", str(rng.randint(1, 1000)))
              msg = msg.replace("{pct}", str(rng.randint(70, 99)))
              msg = msg.replace("{svc}", rng.choice(SERVICES))
              msg = msg.replace("{rid}", hashlib.md5(str(seq).encode()).hexdigest()[:12])
              msg = msg.replace("{n2}", str(rng.randint(1, 50)))
              uid = f"user-{rng.randint(1, 500000)}"
              rid = f"{WORKER:02d}-{seq:012d}"
              ip = f"{rng.randint(10,223)}.{rng.randint(0,255)}.{rng.randint(0,255)}.{rng.randint(1,254)}"
              return {
                  "@timestamp": ts.strftime("%Y-%m-%dT%H:%M:%S.") + f"{rng.randint(0,999):03d}Z",
                  "level": level, "service": svc, "message": msg,
                  "request_id": rid, "user_id": uid,
                  "http": {"method": method, "path": path, "status_code": status,
                           "response_time_ms": rt},
                  "client": {"ip": ip, "user_agent": rng.choice(AGENTS)},
                  "host": f"{svc}-{rng.randint(0,5)}.movie-demo.svc.cluster.local",
                  "trace_id": hashlib.sha256(rid.encode()).hexdigest()[:32],
              }

          def wait_for_es():
              for i in range(120):
                  try:
                      r = urlopen(f"{ES}/_cluster/health?wait_for_status=yellow&timeout=5s", timeout=10)
                      body = r.read()
                      if b'"timed_out":false' in body:
                          print(f"ES ready after {i*2}s")
                          return True
                      print(f"ES not ready yet (attempt {i})...")
                  except Exception as e:
                      if i % 10 == 0:
                          print(f"Waiting for ES (attempt {i}): {e}")
                  time.sleep(2)
              print("ES not ready after 240s, proceeding anyway...")
              return False

          def get_current_doc_count():
              try:
                  r = urlopen(f"{ES}/{INDEX}/_count", timeout=10)
                  return json.loads(r.read()).get("count", 0)
              except:
                  return 0

          def create_index():
              body = json.dumps({
                  "settings": {"number_of_shards": 100, "number_of_replicas": 1,
                               "refresh_interval": "30s",
                               "translog.durability": "async",
                               "translog.sync_interval": "30s",
                               "translog.flush_threshold_size": "1gb"},
                  "mappings": {"properties": {
                      "@timestamp": {"type": "date"},
                      "level": {"type": "keyword"},
                      "service": {"type": "keyword"},
                      "message": {"type": "text"},
                      "request_id": {"type": "keyword"},
                      "user_id": {"type": "keyword"},
                      "http": {"properties": {
                          "method": {"type": "keyword"},
                          "path": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                          "status_code": {"type": "integer"},
                          "response_time_ms": {"type": "integer"}}},
                      "client": {"properties": {
                          "ip": {"type": "ip"},
                          "user_agent": {"type": "text"}}},
                      "host": {"type": "keyword"},
                      "trace_id": {"type": "keyword"},
                  }}
              }).encode()
              try:
                  req = Request(f"{ES}/{INDEX}", data=body, method="PUT",
                                headers={"Content-Type": "application/json"})
                  urlopen(req, timeout=30)
                  print(f"Created index '{INDEX}'")
              except Exception as e:
                  if "resource_already_exists" in str(e).lower() or "400" in str(e):
                      print(f"Index '{INDEX}' already exists")
                  else:
                      print(f"Index creation: {e}")

          def bulk_send(docs):
              body = ""
              for d in docs:
                  body += '{"index":{}}\n' + json.dumps(d, separators=(',', ':')) + "\n"
              data = body.encode()
              for attempt in range(3):
                  try:
                      req = Request(f"{ES}/{INDEX}/_bulk", data=data, method="POST",
                                    headers={"Content-Type": "application/json"})
                      resp = urlopen(req, timeout=120)
                      result = json.loads(resp.read())
                      if result.get("errors"):
                          errs = sum(1 for i in result["items"] if i["index"].get("error"))
                          return len(docs) - errs
                      return len(docs)
                  except Exception as e:
                      if attempt == 2:
                          print(f"  Bulk failed: {e}")
                          return 0
                      time.sleep(2 ** attempt)
              return 0

          # --- Main ---
          print(f"=== Log Generator Worker {WORKER}/{COMPLETIONS} ===")
          print(f"Total target: {TOTAL_TARGET:,} docs across all workers")
          print(f"Throttle: {THROTTLE_MS}ms between batches, batch size {BATCH}")

          print("Waiting for Elasticsearch...")
          wait_for_es()

          if WORKER == 0:
              create_index()
              time.sleep(3)

          time.sleep(WORKER * 2)

          current = get_current_doc_count()
          remaining = max(0, TOTAL_TARGET - current)
          my_share = remaining // COMPLETIONS
          # Last worker picks up remainder
          if WORKER == COMPLETIONS - 1:
              my_share = remaining - (remaining // COMPLETIONS) * (COMPLETIONS - 1)

          print(f"Current docs: {current:,}, remaining: {remaining:,}, my share: {my_share:,}")

          if my_share <= 0:
              print("Target already reached, nothing to do.")
              sys.exit(0)

          total_docs = 0
          seq = WORKER * 500_000_000  # Offset to avoid ID collisions with previous runs
          start = time.time()
          throttle_s = THROTTLE_MS / 1000.0

          while total_docs < my_share:
              batch_size = min(BATCH, my_share - total_docs)
              batch = [make_log(seq + i) for i in range(batch_size)]
              seq += batch_size
              indexed = bulk_send(batch)
              total_docs += indexed

              if throttle_s > 0:
                  time.sleep(throttle_s)

              if total_docs % (BATCH * 100) < BATCH:
                  elapsed = time.time() - start
                  rate = total_docs / elapsed if elapsed > 0 else 0
                  pct = total_docs / my_share * 100
                  print(f"Worker {WORKER}: {total_docs:,}/{my_share:,} ({pct:.1f}%), "
                        f"{rate:,.0f} docs/s")

              if indexed == 0:
                  print(f"Worker {WORKER}: Bulk insert returned 0, pausing 30s...")
                  time.sleep(30)

          elapsed = time.time() - start
          final_count = get_current_doc_count()
          print(f"\n=== Worker {WORKER} Complete ===")
          print(f"Indexed {total_docs:,} docs in {elapsed:.0f}s ({total_docs/elapsed:,.0f} docs/s)")
          print(f"Total index doc count: {final_count:,}")
        resources:
          requests:
            cpu: "2"
            memory: 1Gi
          limits:
            cpu: "4"
            memory: 2Gi
